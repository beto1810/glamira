{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd # type: ignore\n",
    "import re\n",
    "import random\n",
    "import requests # type: ignore\n",
    "import urllib.request\n",
    "from xml.etree import ElementTree as ET\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlencode\n",
    "import concurrent.futures\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract numbers from a string\n",
    "def extract_numbers(class_name):\n",
    "    return re.findall(r'\\d+', class_name)\n",
    "\n",
    "def extract_key(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    path_components = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "    # Use the first component of the path as the key\n",
    "    return path_components[0]\n",
    "\n",
    "# Function to extract category information from a webpage\n",
    "def category_info(link,category_id_list):\n",
    "    try: \n",
    "        session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,  # Number of retries\n",
    "            status_forcelist=[429, 500, 502, 503, 504],  # HTTP statuses to retry on\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]  # Methods to retry\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        source = requests.get(link, timeout=10)  # Adjust timeout as needed\n",
    "        source.raise_for_status()\n",
    "        content = source.content\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Dictionary to store category details\n",
    "        category_dict = {}\n",
    "        \n",
    "\n",
    "        # Find the breadcrumbs div\n",
    "        category_info = soup.find('div', class_='breadcrumbs')\n",
    "\n",
    "        # Extract class names starting with 'category'\n",
    "        class_names = [cls for tag in soup.select('div.breadcrumbs [class]') for cls in tag['class'] if cls.startswith('category')]\n",
    "\n",
    "        # Iterate over class names to extract category details\n",
    "        for category in class_names:\n",
    "            \n",
    "\n",
    "            # Extract the category ID using the helper function\n",
    "            category_id = int(extract_numbers(category)[0]) if extract_numbers(category) else None\n",
    "            if category_id not in category_id_list:\n",
    "                category_id_list.append(category_id)\n",
    "            \n",
    "                # Find the corresponding list item with the category class\n",
    "\n",
    "                category_label = category_info.find('li', class_=f\"item {category}\")\n",
    "                # Determine the category link\n",
    "                if category_label and category_label.find('a') and category_label.find('a')[\"href\"]:\n",
    "                    category_link = category_label.find('a')[\"href\"]\n",
    "                    category_name = category_label.find('a').text.strip() \n",
    "                else:\n",
    "                    category_link = link  # Fallback to the main link if no category link found\n",
    "                    category_name = extract_key(link)\n",
    "                # Extract category name\n",
    "            \n",
    "\n",
    "            # Prepare category details\n",
    "                category_detail = {\n",
    "                    'category_link': category_link,\n",
    "                    'category_id': category_id\n",
    "                }\n",
    "\n",
    "    \n",
    "            # Add to dictionary if not already present\n",
    "                if category_name not in category_dict:\n",
    "                    category_dict[category_name] = category_detail\n",
    "\n",
    "        return category_dict\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {link}: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category details saved to category_details.json\n"
     ]
    }
   ],
   "source": [
    "xml_files = ['https://www.glamira.com/media/sitemap/glus/category_provider.xml']\n",
    "links = []\n",
    "category_dict = {}\n",
    "category_id_list = []\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    response = requests.get(xml_file)\n",
    "    xml_content = response.content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "# Extract all https links\n",
    "\n",
    "    for element in root.iter():\n",
    "        if element.text is not None and element.text.startswith(\"https:\") and \"carat\" not in element.text:\n",
    "            links.append(element.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save results into a final category dictionary\n",
    "final_category_dict = {}\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(category_info, link, category_id_list) for link in links]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            final_category_dict.update(result)\n",
    "\n",
    "# Save final_category_dict to JSON file\n",
    "output_file = 'category_details.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_category_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Category details saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category details saved to category_details.json\n"
     ]
    }
   ],
   "source": [
    "output_file = 'category_details.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_category_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Category details saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
